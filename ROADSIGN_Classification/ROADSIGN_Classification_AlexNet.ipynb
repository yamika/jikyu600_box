{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from load_dataset_VGG16 import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load label : Done!\n",
      "Load img : Done!\n",
      "Load label : Done!\n",
      "Load img : Done!\n"
     ]
    }
   ],
   "source": [
    "x_train,t_train = load_dataset('./train_dataset_AlexNet',convert_type='RGB',flatten=True,normalize=True,one_hot_label=True)\n",
    "x_test,t_test = load_dataset('./test_dataset_AlexNet',convert_type='RGB',flatten=True,normalize=True,one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 224*224*3],name=\"input\")\n",
    "t = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "### 入力層\n",
    "input_layer = tf.reshape(X, [-1,224,224,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 畳込み層、プーリング層1\n",
    "h_conv1 = tf.layers.conv2d(inputs=input_layer,filters=96,kernel_size=[11, 11],strides=(4,4),padding='same',activation=tf.nn.relu)\n",
    "h_norm1 = tf.nn.local_response_normalization(h_conv1,depth_radius=2,alpha=2e-5,beta=0.75)\n",
    "h_pool1 = tf.layers.max_pooling2d(inputs=h_norm1,pool_size=(3,3),strides=(2,2),padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LRN_1:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(?, 14, 14, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### 畳込み層、プーリング層2\n",
    "h_conv2 = tf.layers.conv2d(inputs=h_pool1,filters=256,kernel_size=[5, 5],padding='same',activation=tf.nn.relu)\n",
    "h_norm2 = tf.nn.local_response_normalization(h_conv2,depth_radius=2,alpha=2e-5,beta=0.75)\n",
    "h_pool2 = tf.layers.max_pooling2d(inputs=h_norm2,pool_size=(3,3),strides=(2,2),padding='same')\n",
    "print(h_norm2)\n",
    "print(h_pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_5/Relu:0\", shape=(?, 14, 14, 256), dtype=float32)\n",
      "Tensor(\"max_pooling2d_3/MaxPool:0\", shape=(?, 7, 7, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### 畳込み層、プーリング層3\n",
    "h_conv3_1 = tf.layers.conv2d(inputs=h_pool2,filters=384,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "h_conv3_2 = tf.layers.conv2d(inputs=h_conv3_1,filters=384,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "h_conv3_3 = tf.layers.conv2d(inputs=h_conv3_2,filters=256,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "\n",
    "h_pool3 = tf.layers.max_pooling2d(inputs=h_conv3_3,pool_size=(3,3),strides=(2,2),padding='same')\n",
    "\n",
    "print(h_conv3_3)\n",
    "print(h_pool3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 全結合層1\n",
    "\n",
    "stddev = np.sqrt(1.0 / 7*7*256)\n",
    "h_W_fc1 = tf.Variable(tf.truncated_normal([7*7*256,4096], stddev=stddev)) \n",
    "h_b_fc1 = tf.Variable(tf.constant(0.1, shape=[4096]))\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 7*7*256])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, h_W_fc1) + h_b_fc1)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 全結合層2\n",
    "\n",
    "stddev = np.sqrt(1.0 / 4096)\n",
    "h_W_fc2 = tf.Variable(tf.truncated_normal([4096,4096], stddev=stddev)) \n",
    "h_b_fc2 = tf.Variable(tf.constant(0.1, shape=[4096]))\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, h_W_fc2) + h_b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 出力層\n",
    "stddev = np.sqrt(2.0 / 4096)\n",
    "W_fc3 = tf.Variable(tf.truncated_normal([4096,4], stddev=stddev))\n",
    "b_fc3 = tf.Variable(tf.constant(0.1, shape=[4]))\n",
    "y_conv = tf.nn.xw_plus_b(h_fc2_drop,W_fc3,b_fc3,name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 損失\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y_conv))\n",
    "\n",
    "### 学習op\n",
    "optimizer = tf.train.AdamOptimizer(1e-6)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "### モデルの評価\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(t,1))\n",
    "# 精度\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step: 0, loss: 21.421619, acc: 0.296047, [Test] acc : 0.270270\n",
      "Time : 19.159147\n",
      "[Train] step: 10, loss: 14.798828, acc: 0.384357, [Test] acc : 0.378378\n",
      "Time : 26.510859\n",
      "[Train] step: 20, loss: 11.714077, acc: 0.387721, [Test] acc : 0.405405\n",
      "Time : 33.499298\n",
      "[Train] step: 30, loss: 10.144858, acc: 0.386880, [Test] acc : 0.405405\n",
      "Time : 40.614627\n",
      "[Train] step: 40, loss: 8.788651, acc: 0.412111, [Test] acc : 0.351351\n",
      "Time : 47.799222\n",
      "[Train] step: 50, loss: 8.689365, acc: 0.428091, [Test] acc : 0.405405\n",
      "Time : 54.866660\n",
      "[Train] step: 60, loss: 8.642957, acc: 0.439024, [Test] acc : 0.432432\n",
      "Time : 61.913993\n",
      "[Train] step: 70, loss: 7.969517, acc: 0.444912, [Test] acc : 0.378378\n",
      "Time : 69.240307\n",
      "[Train] step: 80, loss: 8.021721, acc: 0.449117, [Test] acc : 0.378378\n",
      "Time : 76.563772\n",
      "[Train] step: 90, loss: 6.899406, acc: 0.456686, [Test] acc : 0.378378\n",
      "Time : 83.869706\n",
      "[Train] step: 100, loss: 6.428270, acc: 0.460891, [Test] acc : 0.351351\n",
      "Time : 91.168086\n",
      "[Train] step: 110, loss: 5.825630, acc: 0.467620, [Test] acc : 0.378378\n",
      "Time : 98.198490\n",
      "[Train] step: 120, loss: 5.161312, acc: 0.476871, [Test] acc : 0.378378\n",
      "Time : 105.304834\n",
      "[Train] step: 130, loss: 5.173127, acc: 0.475189, [Test] acc : 0.378378\n",
      "Time : 112.523933\n",
      "[Train] step: 140, loss: 5.460159, acc: 0.483600, [Test] acc : 0.378378\n",
      "Time : 119.781610\n",
      "[Train] step: 150, loss: 5.495086, acc: 0.485282, [Test] acc : 0.432432\n",
      "Time : 127.145826\n",
      "[Train] step: 160, loss: 5.239022, acc: 0.484441, [Test] acc : 0.432432\n",
      "Time : 134.487999\n",
      "[Train] step: 170, loss: 5.163359, acc: 0.481077, [Test] acc : 0.405405\n",
      "Time : 141.519356\n",
      "[Train] step: 180, loss: 3.984988, acc: 0.513877, [Test] acc : 0.486486\n",
      "Time : 148.763189\n",
      "[Train] step: 190, loss: 3.632435, acc: 0.540791, [Test] acc : 0.513514\n",
      "Time : 156.151158\n",
      "[Train] step: 200, loss: 3.491443, acc: 0.547519, [Test] acc : 0.513514\n",
      "Time : 163.378438\n",
      "[Train] step: 210, loss: 3.561414, acc: 0.541632, [Test] acc : 0.486486\n",
      "Time : 170.728069\n",
      "[Train] step: 220, loss: 3.629800, acc: 0.537426, [Test] acc : 0.459459\n",
      "Time : 178.002049\n",
      "[Train] step: 230, loss: 3.092832, acc: 0.555088, [Test] acc : 0.540541\n",
      "Time : 185.206898\n",
      "[Train] step: 240, loss: 2.756968, acc: 0.573591, [Test] acc : 0.594595\n",
      "Time : 192.376665\n",
      "[Train] step: 250, loss: 2.717597, acc: 0.577796, [Test] acc : 0.594595\n",
      "Time : 199.734069\n",
      "[Train] step: 260, loss: 2.974782, acc: 0.581161, [Test] acc : 0.567568\n",
      "Time : 207.254138\n",
      "[Train] step: 270, loss: 2.933390, acc: 0.589571, [Test] acc : 0.594595\n",
      "Time : 214.334067\n",
      "[Train] step: 280, loss: 2.866375, acc: 0.592935, [Test] acc : 0.648649\n",
      "Time : 221.581941\n",
      "[Train] step: 290, loss: 2.700058, acc: 0.590412, [Test] acc : 0.621622\n",
      "Time : 228.949804\n",
      "[Train] step: 300, loss: 2.543624, acc: 0.588730, [Test] acc : 0.594595\n",
      "Time : 236.186030\n",
      "[Train] step: 310, loss: 2.415558, acc: 0.599664, [Test] acc : 0.621622\n",
      "Time : 243.401997\n",
      "[Train] step: 320, loss: 2.448549, acc: 0.604710, [Test] acc : 0.648649\n",
      "Time : 250.889243\n",
      "[Train] step: 330, loss: 2.492521, acc: 0.605551, [Test] acc : 0.648649\n",
      "Time : 257.957561\n",
      "[Train] step: 340, loss: 2.271914, acc: 0.614802, [Test] acc : 0.648649\n",
      "Time : 265.054220\n",
      "[Train] step: 350, loss: 2.083496, acc: 0.622372, [Test] acc : 0.648649\n",
      "Time : 272.258116\n",
      "[Train] step: 360, loss: 1.870399, acc: 0.629100, [Test] acc : 0.675676\n",
      "Time : 279.555004\n",
      "[Train] step: 370, loss: 1.881768, acc: 0.634987, [Test] acc : 0.702703\n",
      "Time : 286.799575\n",
      "[Train] step: 380, loss: 1.865954, acc: 0.629941, [Test] acc : 0.702703\n",
      "Time : 293.958385\n",
      "[Train] step: 390, loss: 1.775729, acc: 0.633305, [Test] acc : 0.729730\n",
      "Time : 301.261881\n",
      "[Train] step: 400, loss: 1.867149, acc: 0.645080, [Test] acc : 0.702703\n",
      "Time : 308.675597\n",
      "[Train] step: 410, loss: 2.072422, acc: 0.645921, [Test] acc : 0.702703\n",
      "Time : 315.870533\n",
      "[Train] step: 420, loss: 2.287553, acc: 0.629100, [Test] acc : 0.702703\n",
      "Time : 323.240549\n",
      "[Train] step: 430, loss: 2.062928, acc: 0.650126, [Test] acc : 0.702703\n",
      "Time : 330.544785\n",
      "[Train] step: 440, loss: 1.971322, acc: 0.658537, [Test] acc : 0.702703\n",
      "Time : 337.673297\n",
      "[Train] step: 450, loss: 1.689711, acc: 0.662742, [Test] acc : 0.729730\n",
      "Time : 344.858899\n",
      "[Train] step: 460, loss: 1.699351, acc: 0.665265, [Test] acc : 0.702703\n",
      "Time : 351.916828\n",
      "[Train] step: 470, loss: 1.718214, acc: 0.668629, [Test] acc : 0.702703\n",
      "Time : 359.130837\n",
      "[Train] step: 480, loss: 1.659451, acc: 0.673675, [Test] acc : 0.702703\n",
      "Time : 366.428965\n",
      "[Train] step: 490, loss: 1.646270, acc: 0.676198, [Test] acc : 0.702703\n",
      "Time : 373.468712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./RoadSign-Alexnet-ckpt/roadsign_alex'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "start = time.time()\n",
    "for i in range(500):\n",
    "    batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    sess.run(train_step,feed_dict={X: x_batch, t: t_batch,keep_prob:0.5})\n",
    "    if i % 10 == 0:\n",
    "        train_acc, train_loss = sess.run([accuracy,loss], feed_dict={X: x_train, t: t_train,keep_prob:1.0})\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: x_test, t: t_test,keep_prob:1.0})\n",
    "        print \"[Train] step: %d, loss: %f, acc: %f, [Test] acc : %f\" % (i, train_loss, train_acc,test_acc)\n",
    "        print(\"Time : %f\" % (time.time() - start))\n",
    "        \n",
    "saver.save(sess, \"./RoadSign-Alexnet-ckpt/roadsign_alex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ./RoadSign-Alexnet-ckpt/roadsign_alex\n",
      "INFO:tensorflow:Restoring parameters from ./RoadSign-Alexnet-ckpt/roadsign_alex\n",
      "[Train] step: 0, loss: 1.581534, acc: 0.677040, [Test] acc : 0.702703\n",
      "Time : 2.500137\n",
      "[Train] step: 10, loss: 1.556070, acc: 0.677881, [Test] acc : 0.675676\n",
      "Time : 9.678245\n",
      "[Train] step: 20, loss: 1.649938, acc: 0.673675, [Test] acc : 0.702703\n",
      "Time : 16.767586\n",
      "[Train] step: 30, loss: 1.611284, acc: 0.677040, [Test] acc : 0.702703\n",
      "Time : 23.955387\n",
      "[Train] step: 40, loss: 1.468298, acc: 0.688814, [Test] acc : 0.702703\n",
      "Time : 31.063954\n",
      "[Train] step: 50, loss: 1.416686, acc: 0.693019, [Test] acc : 0.702703\n",
      "Time : 38.321554\n",
      "[Train] step: 60, loss: 1.362910, acc: 0.698066, [Test] acc : 0.702703\n",
      "Time : 45.500528\n",
      "[Train] step: 70, loss: 1.415463, acc: 0.699748, [Test] acc : 0.702703\n",
      "Time : 52.580843\n",
      "[Train] step: 80, loss: 1.516504, acc: 0.688814, [Test] acc : 0.702703\n",
      "Time : 59.697216\n",
      "[Train] step: 90, loss: 1.561564, acc: 0.692178, [Test] acc : 0.702703\n",
      "Time : 66.856037\n",
      "[Train] step: 100, loss: 1.557043, acc: 0.693019, [Test] acc : 0.702703\n",
      "Time : 74.040915\n",
      "[Train] step: 110, loss: 1.328997, acc: 0.706476, [Test] acc : 0.729730\n",
      "Time : 81.269900\n",
      "[Train] step: 120, loss: 1.248048, acc: 0.718251, [Test] acc : 0.648649\n",
      "Time : 88.487367\n",
      "[Train] step: 130, loss: 1.205997, acc: 0.719092, [Test] acc : 0.675676\n",
      "Time : 95.618400\n",
      "[Train] step: 140, loss: 1.172416, acc: 0.722456, [Test] acc : 0.675676\n",
      "Time : 103.033987\n",
      "[Train] step: 150, loss: 1.134571, acc: 0.718251, [Test] acc : 0.675676\n",
      "Time : 110.230979\n",
      "[Train] step: 160, loss: 1.057500, acc: 0.731707, [Test] acc : 0.675676\n",
      "Time : 117.281238\n",
      "[Train] step: 170, loss: 1.033741, acc: 0.733389, [Test] acc : 0.648649\n",
      "Time : 124.494219\n",
      "[Train] step: 180, loss: 0.991659, acc: 0.738436, [Test] acc : 0.675676\n",
      "Time : 131.660091\n",
      "[Train] step: 190, loss: 0.968161, acc: 0.739277, [Test] acc : 0.702703\n",
      "Time : 138.935640\n",
      "[Train] step: 200, loss: 1.007511, acc: 0.738436, [Test] acc : 0.729730\n",
      "Time : 146.253195\n",
      "[Train] step: 210, loss: 0.942077, acc: 0.742641, [Test] acc : 0.729730\n",
      "Time : 153.382268\n",
      "[Train] step: 220, loss: 0.925458, acc: 0.740959, [Test] acc : 0.702703\n",
      "Time : 160.400471\n",
      "[Train] step: 230, loss: 1.011525, acc: 0.735071, [Test] acc : 0.702703\n",
      "Time : 167.427513\n",
      "[Train] step: 240, loss: 1.041984, acc: 0.730025, [Test] acc : 0.702703\n",
      "Time : 174.615015\n",
      "[Train] step: 250, loss: 1.048500, acc: 0.735072, [Test] acc : 0.729730\n",
      "Time : 181.800033\n",
      "[Train] step: 260, loss: 1.022641, acc: 0.735913, [Test] acc : 0.729730\n",
      "Time : 189.052838\n",
      "[Train] step: 270, loss: 0.930413, acc: 0.745164, [Test] acc : 0.702703\n",
      "Time : 196.307476\n",
      "[Train] step: 280, loss: 0.888444, acc: 0.746005, [Test] acc : 0.675676\n",
      "Time : 203.467624\n",
      "[Train] step: 290, loss: 0.843747, acc: 0.750210, [Test] acc : 0.675676\n",
      "Time : 210.725430\n",
      "[Train] step: 300, loss: 0.768162, acc: 0.757780, [Test] acc : 0.675676\n",
      "Time : 217.850528\n",
      "[Train] step: 310, loss: 0.735746, acc: 0.761144, [Test] acc : 0.702703\n",
      "Time : 225.120055\n",
      "[Train] step: 320, loss: 0.735957, acc: 0.761144, [Test] acc : 0.702703\n",
      "Time : 232.261308\n",
      "[Train] step: 330, loss: 0.738654, acc: 0.762826, [Test] acc : 0.729730\n",
      "Time : 239.480114\n",
      "[Train] step: 340, loss: 0.748853, acc: 0.764508, [Test] acc : 0.702703\n",
      "Time : 246.548842\n",
      "[Train] step: 350, loss: 0.722805, acc: 0.762826, [Test] acc : 0.702703\n",
      "Time : 253.619792\n",
      "[Train] step: 360, loss: 0.698178, acc: 0.768713, [Test] acc : 0.675676\n",
      "Time : 260.880283\n",
      "[Train] step: 370, loss: 0.701176, acc: 0.765349, [Test] acc : 0.702703\n",
      "Time : 267.993402\n",
      "[Train] step: 380, loss: 0.731896, acc: 0.762826, [Test] acc : 0.729730\n",
      "Time : 275.200065\n",
      "[Train] step: 390, loss: 0.817426, acc: 0.751051, [Test] acc : 0.702703\n",
      "Time : 282.368692\n",
      "[Train] step: 400, loss: 0.897042, acc: 0.746846, [Test] acc : 0.756757\n",
      "Time : 289.652106\n",
      "[Train] step: 410, loss: 0.799592, acc: 0.761144, [Test] acc : 0.702703\n",
      "Time : 296.813839\n",
      "[Train] step: 420, loss: 0.695757, acc: 0.768713, [Test] acc : 0.729730\n",
      "Time : 304.025595\n",
      "[Train] step: 430, loss: 0.670373, acc: 0.777124, [Test] acc : 0.729730\n",
      "Time : 311.288143\n",
      "[Train] step: 440, loss: 0.682476, acc: 0.776283, [Test] acc : 0.702703\n",
      "Time : 318.616223\n",
      "[Train] step: 450, loss: 0.661684, acc: 0.775442, [Test] acc : 0.756757\n",
      "Time : 325.840689\n",
      "[Train] step: 460, loss: 0.641703, acc: 0.781329, [Test] acc : 0.729730\n",
      "Time : 332.908649\n",
      "[Train] step: 470, loss: 0.635867, acc: 0.781329, [Test] acc : 0.729730\n",
      "Time : 339.949117\n",
      "[Train] step: 480, loss: 0.680612, acc: 0.772077, [Test] acc : 0.729730\n",
      "Time : 346.996625\n",
      "[Train] step: 490, loss: 0.715818, acc: 0.765349, [Test] acc : 0.675676\n",
      "Time : 354.078495\n",
      "[Train] step: 500, loss: 0.655218, acc: 0.775442, [Test] acc : 0.729730\n",
      "Time : 361.346331\n",
      "[Train] step: 510, loss: 0.621231, acc: 0.783011, [Test] acc : 0.729730\n",
      "Time : 368.525363\n",
      "[Train] step: 520, loss: 0.595167, acc: 0.789739, [Test] acc : 0.729730\n",
      "Time : 375.742301\n",
      "[Train] step: 530, loss: 0.617733, acc: 0.785534, [Test] acc : 0.702703\n",
      "Time : 382.887718\n",
      "[Train] step: 540, loss: 0.616668, acc: 0.782170, [Test] acc : 0.729730\n",
      "Time : 390.101980\n",
      "[Train] step: 550, loss: 0.594762, acc: 0.783852, [Test] acc : 0.729730\n",
      "Time : 397.235342\n",
      "[Train] step: 560, loss: 0.570469, acc: 0.791421, [Test] acc : 0.729730\n",
      "Time : 404.300207\n",
      "[Train] step: 570, loss: 0.621688, acc: 0.779647, [Test] acc : 0.756757\n",
      "Time : 411.435336\n",
      "[Train] step: 580, loss: 0.608082, acc: 0.785534, [Test] acc : 0.756757\n",
      "Time : 418.670765\n",
      "[Train] step: 590, loss: 0.545373, acc: 0.798991, [Test] acc : 0.756757\n",
      "Time : 425.874765\n",
      "[Train] step: 600, loss: 0.500177, acc: 0.804878, [Test] acc : 0.729730\n",
      "Time : 433.094109\n",
      "[Train] step: 610, loss: 0.486999, acc: 0.809083, [Test] acc : 0.729730\n",
      "Time : 440.133477\n",
      "[Train] step: 620, loss: 0.507273, acc: 0.806560, [Test] acc : 0.729730\n",
      "Time : 447.232286\n",
      "[Train] step: 630, loss: 0.536627, acc: 0.801514, [Test] acc : 0.756757\n",
      "Time : 454.410315\n",
      "[Train] step: 640, loss: 0.531936, acc: 0.800673, [Test] acc : 0.756757\n",
      "Time : 461.466724\n",
      "[Train] step: 650, loss: 0.507960, acc: 0.806560, [Test] acc : 0.756757\n",
      "Time : 468.671253\n",
      "[Train] step: 660, loss: 0.501535, acc: 0.809083, [Test] acc : 0.756757\n",
      "Time : 475.800158\n",
      "[Train] step: 670, loss: 0.479961, acc: 0.814130, [Test] acc : 0.756757\n",
      "Time : 483.001393\n",
      "[Train] step: 680, loss: 0.502034, acc: 0.809924, [Test] acc : 0.783784\n",
      "Time : 490.172325\n",
      "[Train] step: 690, loss: 0.502863, acc: 0.804878, [Test] acc : 0.756757\n",
      "Time : 497.422633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./RoadSign-Alexnet-ckpt2/roadsign_alex'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.get_checkpoint_state('./RoadSign-Alexnet-ckpt')\n",
    "if ckpt:\n",
    "    # checkpointファイルから最後に保存したモデルへのパスを取得する\n",
    "    last_model = ckpt.model_checkpoint_path\n",
    "    print(\"load {0}\".format(last_model))\n",
    "    # 学習済みモデルを読み込む\n",
    "    saver.restore(sess, last_model)\n",
    "\n",
    "batch_size = 100\n",
    "start = time.time()\n",
    "for i in range(700):\n",
    "    batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    sess.run(train_step,feed_dict={X: x_batch, t: t_batch,keep_prob:0.5})\n",
    "    if i % 10 == 0:\n",
    "        train_acc, train_loss = sess.run([accuracy,loss], feed_dict={X: x_train, t: t_train,keep_prob:1.0})\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: x_test, t: t_test,keep_prob:1.0})\n",
    "        print \"[Train] step: %d, loss: %f, acc: %f, [Test] acc : %f\" % (i, train_loss, train_acc,test_acc)\n",
    "        print(\"Time : %f\" % (time.time() - start))\n",
    "        \n",
    "saver.save(sess, \"./RoadSign-Alexnet-ckpt2/roadsign_alex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
