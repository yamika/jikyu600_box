{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = tf.placeholder(tf.float32, shape=(None,10))\n",
    "X = tf.placeholder(tf.float32, shape=(None,784),name=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(X, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv1_1 = tf.layers.conv2d(inputs=input_layer,filters=16,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "conv1_2 = tf.layers.conv2d(inputs=conv1_1,filters=16,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1_2,pool_size=(2,2),strides=(2,2),padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2_1 = tf.layers.conv2d(inputs=pool1,filters=32,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "conv2_2 = tf.layers.conv2d(inputs=conv2_1,filters=32,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2_2,pool_size=(2,2),strides=(2,2),padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv3_1 = tf.layers.conv2d(inputs=pool2,filters=64,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "conv3_2 = tf.layers.conv2d(inputs=conv3_1,filters=64,kernel_size=[3, 3],padding='same',activation=tf.nn.relu)\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3_2,pool_size=(2,2),strides=(2,2),padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 14, 14, 16)\n",
      "(?, 7, 7, 32)\n",
      "(?, 4, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "print(pool1.shape)\n",
    "print(pool2.shape)\n",
    "print(pool3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stddev = np.sqrt(2.0 / 64*4*4)\n",
    "h_W_fc1 = tf.Variable(tf.truncated_normal([4*4*64,50], stddev=stddev))\n",
    "h_b_fc1 = tf.Variable(tf.constant(0.1, shape=[50]))\n",
    "pool3_flat = tf.reshape(pool3, [-1, 4*4*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(pool3_flat, h_W_fc1) + h_b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob_1 = tf.placeholder(tf.float32) # ドロップアウトする割合\n",
    "h_drop = tf.nn.dropout(h_fc1, keep_prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stddev = np.sqrt(2.0 / 50)\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([50,10], stddev=stddev))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "fc = tf.nn.relu(tf.matmul(h_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob_2 = tf.placeholder(tf.float32) # ドロップアウトする割合\n",
    "fc2_drop = tf.nn.dropout(fc, keep_prob_2)\n",
    "y_conv = tf.nn.softmax(fc2_drop,name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_term = tf.nn.l2_loss(h_W_fc1) + tf.nn.l2_loss(W_fc2)\n",
    "lambda_ = 0.001\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=t))\n",
    "loss = cross_entropy + lambda_*norm_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(t,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step: 1000, loss: 9.788739, acc: 0.900000\n",
      "[Test] loss: 9.722002, acc: 0.750000\n",
      "[Train] step: 2000, loss: 8.027383, acc: 1.000000\n",
      "[Test] loss: 8.132967, acc: 0.930000\n",
      "[Train] step: 3000, loss: 7.036375, acc: 0.800000\n",
      "[Test] loss: 6.944871, acc: 0.910000\n",
      "[Train] step: 4000, loss: 5.893494, acc: 1.000000\n",
      "[Test] loss: 5.995041, acc: 0.940000\n",
      "[Train] step: 5000, loss: 5.232647, acc: 0.900000\n",
      "[Test] loss: 5.227472, acc: 0.980000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "### 学習の実行\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "i = 0\n",
    "for _ in range(5000):\n",
    "    i += 1\n",
    "    batch = mnist.train.next_batch(10)\n",
    "    sess.run(train_step, feed_dict={X:batch[0],t:batch[1],keep_prob_1:0.5,keep_prob_2:0.5})\n",
    "    if i % 1000 == 0:\n",
    "        train_acc, train_loss = sess.run([accuracy,loss], feed_dict={X:batch[0],t:batch[1],keep_prob_1:1.0,keep_prob_2:1.0})\n",
    "        print \"[Train] step: %d, loss: %f, acc: %f\" % (i, train_loss, train_acc)\n",
    "        # テストデータによるモデルの評価\n",
    "        test_acc, test_loss = sess.run([accuracy,loss], feed_dict={X:mnist.test.images[:100],t:mnist.test.labels[:100],keep_prob_1:1.0,keep_prob_2:1.0})\n",
    "        print \"[Test] loss: %f, acc: %f\" % (test_loss, test_acc)\n",
    "        row = \"%d,%f,%f,%f,%f\\n\" % (i, train_loss, train_acc, test_loss, test_acc)\n",
    "        with open(\"evaluation.csv\", \"a\") as fout:\n",
    "            fout.write(row)\n",
    "\n",
    "saver.save(sess, \"./mnist-ckpt/mnist-conv\")\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
