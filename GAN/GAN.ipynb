{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0fea759fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAXWwobVmgyJNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fec880f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.array(mnist.train.images[753])\n",
    "plt.imshow(img.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(X, scale, offset, axes, is_train):\n",
    "    # 予測のときにはそのまんまの値を返す\n",
    "    if is_train is False:\n",
    "        return X\n",
    "    \n",
    "    epsilon = 1e-5\n",
    "    mean, variance = tf.nn.moments(X, axes)\n",
    "    return tf.nn.batch_normalization(X, mean, variance, offset, scale, epsilon)\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Generator parameter        \n",
    "        self.gen_w1 = tf.Variable(tf.truncated_normal(shape=[100,128],stddev=0.02,dtype=tf.float32),name=\"gen_w1\")\n",
    "        self.gen_b1 = tf.Variable(tf.truncated_normal(shape=[128],stddev=0.02,dtype=tf.float32),name=\"gen_b1\")\n",
    "        \n",
    "        self.gen_w2 = tf.Variable(tf.truncated_normal([128,256],stddev=0.02,dtype=tf.float32),name=\"gen_w2\")\n",
    "        self.gen_b2 = tf.Variable(tf.truncated_normal(shape=[256],stddev=0.02,dtype=tf.float32),name=\"gen_b2\")\n",
    "        \n",
    "        self.gen_w3 = tf.Variable(tf.truncated_normal([256,512],stddev=0.02,dtype=tf.float32),name=\"gen_w3\")\n",
    "        self.gen_b3 = tf.Variable(tf.truncated_normal(shape=[512],stddev=0.02,dtype=tf.float32),name=\"gen_b3\")\n",
    "        \n",
    "        self.gen_w4 = tf.Variable(tf.truncated_normal([512,28*28],stddev=0.02,dtype=tf.float32),name=\"gen_w4\")\n",
    "        self.gen_b4 = tf.Variable(tf.truncated_normal(shape=[28*28],stddev=0.02,dtype=tf.float32),name=\"gen_b4\")\n",
    "        \n",
    "        self.gen_scale_w1 = tf.Variable(tf.ones([128]),name=\"gen_scale_w1\")\n",
    "        self.gen_offset_w1 = tf.Variable(tf.zeros([128]),name=\"gen_offset_w1\")\n",
    "        \n",
    "        self.gen_scale_w2 = tf.Variable(tf.ones([256]),name=\"gen_scale_w2\")\n",
    "        self.gen_offset_w2 = tf.Variable(tf.zeros([256]),name=\"gen_offset_w2\")\n",
    "        \n",
    "        self.gen_scale_w3 = tf.Variable(tf.ones([512]),name=\"gen_scale_w3\")\n",
    "        self.gen_offset_w3 = tf.Variable(tf.zeros([512]),name=\"gen_offset_w3\")\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "    def run(self,z,is_train):\n",
    "        \n",
    "        h1 = tf.nn.leaky_relu(tf.nn.xw_plus_b(z,self.gen_w1,self.gen_b1),alpha=0.2)\n",
    "        h1 = batch_norm(h1,self.gen_scale_w1,self.gen_offset_w1,[0],is_train)\n",
    "        \n",
    "        h2 = tf.nn.leaky_relu(tf.nn.xw_plus_b(h1,self.gen_w2,self.gen_b2),alpha=0.2)\n",
    "        h2 = batch_norm(h2,self.gen_scale_w2,self.gen_offset_w2,[0],is_train)\n",
    "        \n",
    "        h3 = tf.nn.leaky_relu(tf.nn.xw_plus_b(h2,self.gen_w3,self.gen_b3),alpha=0.2)\n",
    "        h3 = batch_norm(h3,self.gen_scale_w3,self.gen_offset_w3,[0],is_train)\n",
    "        h3_drop = tf.nn.dropout(h3, self.keep_prob)\n",
    "        \n",
    "        fc = tf.nn.sigmoid(tf.nn.xw_plus_b(h3_drop,self.gen_w4,self.gen_b4))\n",
    "        \n",
    "        return fc\n",
    "\n",
    "class Discrimitor():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Discrimitor parameter\n",
    "        self.dis_w1 = tf.Variable(tf.truncated_normal([28*28,512],stddev=0.02,dtype=tf.float32),name=\"dis_w1\")\n",
    "        self.dis_b1 = tf.Variable(tf.truncated_normal([512],stddev=0.02,dtype=tf.float32),name=\"dis_b1\")\n",
    "        \n",
    "        self.dis_w2 = tf.Variable(tf.truncated_normal([512,256],stddev=0.02,dtype=tf.float32),name=\"dis_w2\")\n",
    "        self.dis_b2 = tf.Variable(tf.truncated_normal([256],stddev=0.02,dtype=tf.float32),name=\"dis_b2\")\n",
    "        \n",
    "        self.dis_w3 = tf.Variable(tf.truncated_normal([256,128],stddev=0.02,dtype=tf.float32),name=\"dis_w3\")\n",
    "        self.dis_b3 = tf.Variable(tf.truncated_normal([128],stddev=0.02,dtype=tf.float32),name=\"dis_b3\")\n",
    "        \n",
    "        self.dis_w4 = tf.Variable(tf.truncated_normal([128,1],stddev=0.02,dtype=tf.float32),name=\"dis_w4\")\n",
    "        self.dis_b4 = tf.Variable(tf.truncated_normal([1],stddev=0.02,dtype=tf.float32),name=\"dis_b4\")\n",
    "        \n",
    "    def run(self,x):\n",
    "\n",
    "        h1 = tf.nn.leaky_relu(tf.nn.xw_plus_b(x,self.dis_w1,self.dis_b1),alpha=0.2)\n",
    "        h2 = tf.nn.leaky_relu(tf.nn.xw_plus_b(h1,self.dis_w2,self.dis_b2),alpha=0.2)   \n",
    "        h3 = tf.nn.leaky_relu(tf.nn.xw_plus_b(h2,self.dis_w3,self.dis_b3),alpha=0.2)\n",
    "        \n",
    "        fc = tf.nn.xw_plus_b(h3,self.dis_w4,self.dis_b4)\n",
    "        \n",
    "        return fc\n",
    "    \n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.input_X = tf.placeholder(tf.float32, shape=(None,28*28))\n",
    "        \n",
    "        # t0は0のラベルを格納し、t1は1のラベルを格納する\n",
    "        self.label_t0 = tf.placeholder(tf.float32, shape=(None,1))\n",
    "        self.label_t1 = tf.placeholder(tf.float32, shape=(None,1))\n",
    "        \n",
    "        # Generator\n",
    "        self.generator = Generator()\n",
    "        # 生成モデルに必要なノイズの入れ物\n",
    "        self.gen_z = tf.placeholder(tf.float32, shape=(None,100))\n",
    "        \n",
    "        # Discrimitor \n",
    "        self.discrimitor = Discrimitor()\n",
    "        \n",
    "        # weight decay\n",
    "        gen_norm_term = tf.nn.l2_loss(self.generator.gen_w2) + tf.nn.l2_loss(self.generator.gen_w3) + tf.nn.l2_loss(self.generator.gen_w4)\n",
    "        gen_lambda_ = 0.001\n",
    "        \n",
    "        dis_norm_term = tf.nn.l2_loss(self.discrimitor.dis_w2) + tf.nn.l2_loss(self.discrimitor.dis_w3)\n",
    "        dis_lambda_ = 0.001\n",
    "        \n",
    "        # 訓練データの識別予測\n",
    "        input_X = self.discrimitor.run(self.input_X)\n",
    "        # 生成されたデータの識別yosoku\n",
    "        generated_X = self.discrimitor.run(self.generator.run(self.gen_z,self.is_train))\n",
    "        \n",
    "        self.dis_entropy_X = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_t1, logits=input_X)  \n",
    "        self.dis_entropy_G = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_t0, logits=generated_X)   \n",
    "        self.dis_loss = tf.reduce_mean(self.dis_entropy_X + self.dis_entropy_G) + dis_norm_term*dis_lambda_\n",
    "        \n",
    "        self.gen_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_t1,logits=generated_X)\n",
    "        self.gen_loss = tf.reduce_mean(self.gen_entropy) + gen_norm_term*gen_lambda_\n",
    "        \n",
    "        # 最適化する際にDならDのみのパラメータを、GならGのみのパラメータを更新するようにしたいのでモデル別の変数を取得する\n",
    "        dis_vars = [x for x in tf.trainable_variables() if \"dis_\" in x.name]\n",
    "        gen_vars = [x for x in tf.trainable_variables() if \"gen_\" in x.name]\n",
    "        \n",
    "        # 識別モデルDの最適化\n",
    "        self.opt_d = tf.train.MomentumOptimizer(0.1, momentum=0.5).minimize(self.dis_loss,var_list=[dis_vars])\n",
    "        # 生成モデルGの最適化\n",
    "        self.opt_g = tf.train.MomentumOptimizer(0.1, momentum=0.5).minimize(self.gen_loss,var_list=[gen_vars])\n",
    "        \n",
    "    def train(self\n",
    "                  , X_train = None \n",
    "                  , batch_size = 100\n",
    "                  , epoch_num = 1000\n",
    "                  , savepath = './mnist_GAN/'\n",
    "                  , init = False):\n",
    "        \n",
    "        if X_train is None:\n",
    "            raise TypeError(\"X_train is None\")\n",
    "\n",
    "        # 訓練途中で生成データを作成して保存したいのでその保存先の作成\n",
    "        p = Path(savepath)\n",
    "        if not(p.is_dir()):\n",
    "            p.mkdir()\n",
    "\n",
    "        sess = tf.Session()\n",
    "        if(init):\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        step = len(X_train) // batch_size\n",
    "        #step = mnist.train.num_examples // batch_size\n",
    "        \n",
    "        # 正解ラベルのミニバッチ\n",
    "        t1_batch = np.ones((batch_size,1),dtype=np.float32)\n",
    "        t0_batch = np.zeros((batch_size,1),dtype=np.float32)\n",
    "        \n",
    "        for epoch in range(epoch_num):\n",
    "            \n",
    "            perm = np.random.permutation(len(X_train))\n",
    "            for k in range(step):\n",
    "                #X_batch = mnist.train.next_batch(batch_size)[0] /255.\n",
    "                X_batch = X_train[perm][k*batch_size:(k+1)*batch_size]\n",
    "                \n",
    "                # Train Discrimitor\n",
    "                # ノイズ事前分布からノイズをミニバッチ分取得\n",
    "                noise_z = np.random.uniform(0,1, size=[batch_size, 100]).astype(np.float32)\n",
    "\n",
    "                sess.run(self.opt_d, feed_dict = {self.input_X:X_batch\n",
    "                                                                          , self.is_train:False\n",
    "                                                                          , self.gen_z:noise_z\n",
    "                                                                          , self.generator.keep_prob:1.0\n",
    "                                                                          , self.label_t1:t1_batch\n",
    "                                                                          , self.label_t0:t0_batch})\n",
    "                \n",
    "                #if k % (step//10) == 0:\n",
    "                # Train Generator\n",
    "                # ノイズ事前分布からノイズをミニバッチ分取得\n",
    "                noise_z = np.random.uniform(0,1, size=[batch_size, 100]).astype(np.float32)\n",
    "                sess.run(self.opt_g, feed_dict = {self.gen_z:noise_z\n",
    "                                                                          , self.is_train:True\n",
    "                                                                          , self.generator.keep_prob:0.5\n",
    "                                                                          , self.label_t1:t1_batch})\n",
    "            \n",
    "            noise_z = np.random.uniform(0,1, size=[batch_size, 100]).astype(np.float32)\n",
    "            train_dis_loss = sess.run(self.dis_loss, feed_dict = {self.input_X:X_batch\n",
    "                                                                                                      , self.is_train:False\n",
    "                                                                                                      , self.gen_z:noise_z\n",
    "                                                                                                      , self.generator.keep_prob:1.0\n",
    "                                                                                                      , self.label_t1:t1_batch\n",
    "                                                                                                      , self.label_t0:t0_batch})\n",
    "\n",
    "            train_gen_loss = sess.run(self.gen_loss, feed_dict ={self.gen_z:noise_z\n",
    "                                                                                                      , self.is_train:False\n",
    "                                                                                                      , self.generator.keep_prob:1.0\n",
    "                                                                                                      , self.label_t1:t1_batch})\n",
    "\n",
    "            print(\"[Train] epoch: %d, dis loss: %f , gen loss : %f\" % (epoch, train_dis_loss, train_gen_loss))\n",
    "            \n",
    "            # 10epoch終了毎に生成モデルから1枚の画像を生成する\n",
    "            if epoch % 10 == 0:\n",
    "                noise_z = np.random.uniform(0,1, size=[1, 100]).astype(np.float32)\n",
    "\n",
    "                z_const = tf.constant(noise_z,dtype=tf.float32)\n",
    "                gen_imgs = sess.run(self.generator.run(z_const, is_train=False),feed_dict={self.generator.keep_prob:1.0}) * 255\n",
    "                Image.fromarray(gen_imgs[0].reshape(28,28)).convert('RGB').save(\n",
    "                        str(p.absolute())+'/generate_img_epoch{0}.jpg'.format(epoch)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 任意の数字のみを学習させたいとき用\n",
    "X_train = np.r_[mnist.train.images[np.where(mnist.train.labels[:,5]  > 0)] ,mnist.test.images[np.where(mnist.test.labels[:,5]  > 0)]] / 255.\n",
    "\n",
    "# 全体を学習したい用\n",
    "#X_train = np.r_[mnist.train.images,mnist.test.images] / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0, dis loss: 0.078119 , gen loss : 7.680473\n",
      "[Train] epoch: 1, dis loss: 0.085785 , gen loss : 3.561870\n",
      "[Train] epoch: 2, dis loss: 1.359633 , gen loss : 0.912048\n",
      "[Train] epoch: 3, dis loss: 1.401109 , gen loss : 0.896579\n",
      "[Train] epoch: 4, dis loss: 1.387708 , gen loss : 0.903403\n",
      "[Train] epoch: 5, dis loss: 1.385955 , gen loss : 0.903940\n",
      "[Train] epoch: 6, dis loss: 1.375986 , gen loss : 0.901337\n",
      "[Train] epoch: 7, dis loss: 1.417405 , gen loss : 0.876229\n",
      "[Train] epoch: 8, dis loss: 1.422792 , gen loss : 0.866282\n",
      "[Train] epoch: 9, dis loss: 1.429992 , gen loss : 0.867796\n",
      "[Train] epoch: 10, dis loss: 1.417564 , gen loss : 0.873641\n",
      "[Train] epoch: 11, dis loss: 1.408666 , gen loss : 0.874550\n",
      "[Train] epoch: 12, dis loss: 1.398145 , gen loss : 0.875390\n",
      "[Train] epoch: 13, dis loss: 1.388433 , gen loss : 0.890254\n",
      "[Train] epoch: 14, dis loss: 1.413883 , gen loss : 0.862240\n",
      "[Train] epoch: 15, dis loss: 1.406475 , gen loss : 0.863806\n",
      "[Train] epoch: 16, dis loss: 1.405181 , gen loss : 0.861191\n",
      "[Train] epoch: 17, dis loss: 1.397638 , gen loss : 0.863789\n",
      "[Train] epoch: 18, dis loss: 1.410287 , gen loss : 0.853694\n",
      "[Train] epoch: 19, dis loss: 1.439368 , gen loss : 0.830718\n",
      "[Train] epoch: 20, dis loss: 1.402085 , gen loss : 0.852527\n",
      "[Train] epoch: 21, dis loss: 1.403830 , gen loss : 0.849112\n",
      "[Train] epoch: 22, dis loss: 1.410315 , gen loss : 0.841703\n",
      "[Train] epoch: 23, dis loss: 1.403889 , gen loss : 0.843643\n",
      "[Train] epoch: 24, dis loss: 1.399528 , gen loss : 0.843936\n",
      "[Train] epoch: 25, dis loss: 1.403909 , gen loss : 0.838449\n",
      "[Train] epoch: 26, dis loss: 1.396467 , gen loss : 0.840652\n",
      "[Train] epoch: 27, dis loss: 1.411026 , gen loss : 0.829156\n",
      "[Train] epoch: 28, dis loss: 1.400951 , gen loss : 0.834080\n",
      "[Train] epoch: 29, dis loss: 1.401544 , gen loss : 0.838555\n",
      "[Train] epoch: 30, dis loss: 1.399001 , gen loss : 0.830552\n",
      "[Train] epoch: 31, dis loss: 1.398848 , gen loss : 0.828747\n",
      "[Train] epoch: 32, dis loss: 1.398107 , gen loss : 0.826970\n",
      "[Train] epoch: 33, dis loss: 1.398779 , gen loss : 0.824216\n",
      "[Train] epoch: 34, dis loss: 1.396258 , gen loss : 0.823608\n",
      "[Train] epoch: 35, dis loss: 1.396111 , gen loss : 0.821560\n",
      "[Train] epoch: 36, dis loss: 1.398016 , gen loss : 0.818472\n",
      "[Train] epoch: 37, dis loss: 1.394002 , gen loss : 0.819538\n",
      "[Train] epoch: 38, dis loss: 1.393024 , gen loss : 0.818764\n",
      "[Train] epoch: 39, dis loss: 1.396089 , gen loss : 0.815450\n",
      "[Train] epoch: 40, dis loss: 1.393705 , gen loss : 0.815597\n",
      "[Train] epoch: 41, dis loss: 1.391161 , gen loss : 0.815190\n",
      "[Train] epoch: 42, dis loss: 1.390584 , gen loss : 0.815412\n",
      "[Train] epoch: 43, dis loss: 1.394588 , gen loss : 0.810101\n",
      "[Train] epoch: 44, dis loss: 1.392199 , gen loss : 0.810404\n",
      "[Train] epoch: 45, dis loss: 1.396307 , gen loss : 0.806384\n",
      "[Train] epoch: 46, dis loss: 1.393521 , gen loss : 0.807015\n",
      "[Train] epoch: 47, dis loss: 1.391312 , gen loss : 0.806151\n",
      "[Train] epoch: 48, dis loss: 1.395493 , gen loss : 0.803269\n",
      "[Train] epoch: 49, dis loss: 1.391625 , gen loss : 0.804272\n",
      "[Train] epoch: 50, dis loss: 1.389989 , gen loss : 0.801111\n",
      "[Train] epoch: 51, dis loss: 1.391691 , gen loss : 0.801508\n",
      "[Train] epoch: 52, dis loss: 1.391533 , gen loss : 0.799961\n",
      "[Train] epoch: 53, dis loss: 1.387448 , gen loss : 0.806414\n",
      "[Train] epoch: 54, dis loss: 1.394472 , gen loss : 0.796181\n",
      "[Train] epoch: 55, dis loss: 1.415335 , gen loss : 0.788303\n",
      "[Train] epoch: 56, dis loss: 1.392805 , gen loss : 0.795497\n",
      "[Train] epoch: 57, dis loss: 1.391513 , gen loss : 0.795503\n",
      "[Train] epoch: 58, dis loss: 1.390494 , gen loss : 0.794693\n",
      "[Train] epoch: 59, dis loss: 1.391913 , gen loss : 0.792907\n",
      "[Train] epoch: 60, dis loss: 1.388620 , gen loss : 0.794103\n",
      "[Train] epoch: 61, dis loss: 1.390431 , gen loss : 0.792073\n",
      "[Train] epoch: 62, dis loss: 1.393639 , gen loss : 0.788606\n",
      "[Train] epoch: 63, dis loss: 1.398335 , gen loss : 0.783146\n",
      "[Train] epoch: 64, dis loss: 1.390058 , gen loss : 0.788581\n",
      "[Train] epoch: 65, dis loss: 1.461296 , gen loss : 0.732358\n",
      "[Train] epoch: 66, dis loss: 1.391333 , gen loss : 0.786588\n",
      "[Train] epoch: 67, dis loss: 1.390766 , gen loss : 0.784793\n",
      "[Train] epoch: 68, dis loss: 1.389066 , gen loss : 0.785988\n",
      "[Train] epoch: 69, dis loss: 1.390880 , gen loss : 0.783760\n",
      "[Train] epoch: 70, dis loss: 1.389459 , gen loss : 0.783761\n",
      "[Train] epoch: 71, dis loss: 1.388780 , gen loss : 0.783354\n",
      "[Train] epoch: 72, dis loss: 1.391455 , gen loss : 0.780752\n",
      "[Train] epoch: 73, dis loss: 1.394930 , gen loss : 0.782513\n",
      "[Train] epoch: 74, dis loss: 1.395245 , gen loss : 0.780670\n",
      "[Train] epoch: 75, dis loss: 1.395036 , gen loss : 0.779599\n",
      "[Train] epoch: 76, dis loss: 1.396547 , gen loss : 0.777408\n",
      "[Train] epoch: 77, dis loss: 1.392796 , gen loss : 0.778813\n",
      "[Train] epoch: 78, dis loss: 1.393804 , gen loss : 0.777153\n",
      "[Train] epoch: 79, dis loss: 1.404751 , gen loss : 0.767761\n",
      "[Train] epoch: 80, dis loss: 1.392933 , gen loss : 0.775926\n",
      "[Train] epoch: 81, dis loss: 1.390934 , gen loss : 0.776389\n",
      "[Train] epoch: 82, dis loss: 1.392851 , gen loss : 0.779023\n",
      "[Train] epoch: 83, dis loss: 1.391700 , gen loss : 0.773862\n",
      "[Train] epoch: 84, dis loss: 1.390131 , gen loss : 0.773899\n",
      "[Train] epoch: 85, dis loss: 1.391462 , gen loss : 0.772166\n",
      "[Train] epoch: 86, dis loss: 1.391421 , gen loss : 0.771300\n",
      "[Train] epoch: 87, dis loss: 1.388905 , gen loss : 0.772278\n",
      "[Train] epoch: 88, dis loss: 1.388556 , gen loss : 0.771500\n",
      "[Train] epoch: 89, dis loss: 1.388243 , gen loss : 0.771566\n",
      "[Train] epoch: 90, dis loss: 1.387669 , gen loss : 0.771171\n",
      "[Train] epoch: 91, dis loss: 1.386532 , gen loss : 0.770915\n",
      "[Train] epoch: 92, dis loss: 1.385579 , gen loss : 0.771048\n",
      "[Train] epoch: 93, dis loss: 1.391454 , gen loss : 0.766659\n",
      "[Train] epoch: 94, dis loss: 1.389049 , gen loss : 0.767526\n",
      "[Train] epoch: 95, dis loss: 1.390496 , gen loss : 0.766049\n",
      "[Train] epoch: 96, dis loss: 1.390510 , gen loss : 0.765416\n",
      "[Train] epoch: 97, dis loss: 1.390553 , gen loss : 0.764524\n",
      "[Train] epoch: 98, dis loss: 1.388972 , gen loss : 0.765211\n",
      "[Train] epoch: 99, dis loss: 1.387936 , gen loss : 0.765135\n",
      "[Train] epoch: 100, dis loss: 1.387178 , gen loss : 0.765250\n",
      "[Train] epoch: 101, dis loss: 1.385791 , gen loss : 0.765486\n",
      "[Train] epoch: 102, dis loss: 1.385806 , gen loss : 0.764773\n",
      "[Train] epoch: 103, dis loss: 1.385312 , gen loss : 0.764565\n",
      "[Train] epoch: 104, dis loss: 1.383423 , gen loss : 0.765755\n",
      "[Train] epoch: 105, dis loss: 1.383316 , gen loss : 0.765319\n",
      "[Train] epoch: 106, dis loss: 1.384576 , gen loss : 0.767889\n",
      "[Train] epoch: 107, dis loss: 1.385029 , gen loss : 0.763672\n",
      "[Train] epoch: 108, dis loss: 1.384203 , gen loss : 0.764092\n",
      "[Train] epoch: 109, dis loss: 1.382483 , gen loss : 0.764155\n",
      "[Train] epoch: 110, dis loss: 1.380501 , gen loss : 0.765418\n",
      "[Train] epoch: 111, dis loss: 1.378694 , gen loss : 0.766030\n",
      "[Train] epoch: 112, dis loss: 1.416137 , gen loss : 0.763569\n",
      "[Train] epoch: 113, dis loss: 1.385702 , gen loss : 0.763029\n",
      "[Train] epoch: 114, dis loss: 1.384814 , gen loss : 0.762314\n",
      "[Train] epoch: 115, dis loss: 1.381045 , gen loss : 0.764378\n",
      "[Train] epoch: 116, dis loss: 1.381976 , gen loss : 0.763284\n",
      "[Train] epoch: 117, dis loss: 1.383373 , gen loss : 0.694826\n",
      "[Train] epoch: 118, dis loss: 1.384053 , gen loss : 0.688148\n",
      "[Train] epoch: 119, dis loss: 1.374955 , gen loss : 0.773485\n",
      "[Train] epoch: 120, dis loss: 1.369402 , gen loss : 0.780367\n",
      "[Train] epoch: 121, dis loss: 1.384662 , gen loss : 0.834235\n",
      "[Train] epoch: 122, dis loss: 1.377742 , gen loss : 0.767765\n",
      "[Train] epoch: 123, dis loss: 1.373874 , gen loss : 0.753729\n",
      "[Train] epoch: 124, dis loss: 1.369334 , gen loss : 0.786509\n",
      "[Train] epoch: 125, dis loss: 1.376614 , gen loss : 0.684675\n",
      "[Train] epoch: 126, dis loss: 1.477058 , gen loss : 0.484809\n",
      "[Train] epoch: 127, dis loss: 1.370772 , gen loss : 0.750343\n",
      "[Train] epoch: 128, dis loss: 1.364202 , gen loss : 0.767777\n",
      "[Train] epoch: 129, dis loss: 1.360783 , gen loss : 0.780942\n",
      "[Train] epoch: 130, dis loss: 1.371686 , gen loss : 0.911970\n",
      "[Train] epoch: 131, dis loss: 1.357240 , gen loss : 0.757018\n",
      "[Train] epoch: 132, dis loss: 1.356151 , gen loss : 0.772498\n",
      "[Train] epoch: 133, dis loss: 1.354187 , gen loss : 0.777294\n",
      "[Train] epoch: 134, dis loss: 1.349435 , gen loss : 0.780538\n",
      "[Train] epoch: 135, dis loss: 1.350228 , gen loss : 0.776547\n",
      "[Train] epoch: 136, dis loss: 1.346217 , gen loss : 0.752474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 137, dis loss: 1.387306 , gen loss : 0.758596\n",
      "[Train] epoch: 138, dis loss: 1.381018 , gen loss : 0.759950\n",
      "[Train] epoch: 139, dis loss: 1.377271 , gen loss : 0.761213\n",
      "[Train] epoch: 140, dis loss: 1.373236 , gen loss : 0.760845\n",
      "[Train] epoch: 141, dis loss: 1.433924 , gen loss : 1.058788\n",
      "[Train] epoch: 142, dis loss: 1.373551 , gen loss : 0.761189\n",
      "[Train] epoch: 143, dis loss: 1.472109 , gen loss : 0.668246\n",
      "[Train] epoch: 144, dis loss: 1.387705 , gen loss : 0.731920\n",
      "[Train] epoch: 145, dis loss: 1.385282 , gen loss : 0.754760\n",
      "[Train] epoch: 146, dis loss: 1.382266 , gen loss : 0.756895\n",
      "[Train] epoch: 147, dis loss: 1.377910 , gen loss : 0.757544\n",
      "[Train] epoch: 148, dis loss: 1.381461 , gen loss : 0.745199\n",
      "[Train] epoch: 149, dis loss: 1.376939 , gen loss : 0.757952\n",
      "[Train] epoch: 150, dis loss: 1.374464 , gen loss : 0.759460\n",
      "[Train] epoch: 151, dis loss: 1.383846 , gen loss : 0.752413\n",
      "[Train] epoch: 152, dis loss: 1.381500 , gen loss : 0.753606\n",
      "[Train] epoch: 153, dis loss: 1.378426 , gen loss : 0.754296\n",
      "[Train] epoch: 154, dis loss: 1.377756 , gen loss : 0.755147\n",
      "[Train] epoch: 155, dis loss: 1.376091 , gen loss : 0.755572\n",
      "[Train] epoch: 156, dis loss: 1.367217 , gen loss : 0.758464\n",
      "[Train] epoch: 157, dis loss: 1.365610 , gen loss : 0.760681\n",
      "[Train] epoch: 158, dis loss: 1.395440 , gen loss : 0.685853\n",
      "[Train] epoch: 159, dis loss: 1.390545 , gen loss : 0.746789\n",
      "[Train] epoch: 160, dis loss: 1.389454 , gen loss : 0.746371\n",
      "[Train] epoch: 161, dis loss: 1.389723 , gen loss : 0.744887\n",
      "[Train] epoch: 162, dis loss: 1.389644 , gen loss : 0.743944\n",
      "[Train] epoch: 163, dis loss: 1.389598 , gen loss : 0.743056\n",
      "[Train] epoch: 164, dis loss: 1.389111 , gen loss : 0.742200\n",
      "[Train] epoch: 165, dis loss: 1.388965 , gen loss : 0.741458\n",
      "[Train] epoch: 166, dis loss: 1.388653 , gen loss : 0.740732\n",
      "[Train] epoch: 167, dis loss: 1.388011 , gen loss : 0.740146\n",
      "[Train] epoch: 168, dis loss: 1.387884 , gen loss : 0.739593\n",
      "[Train] epoch: 169, dis loss: 1.387427 , gen loss : 0.739121\n",
      "[Train] epoch: 170, dis loss: 1.387136 , gen loss : 0.738711\n",
      "[Train] epoch: 171, dis loss: 1.386523 , gen loss : 0.738386\n",
      "[Train] epoch: 172, dis loss: 1.386011 , gen loss : 0.738052\n",
      "[Train] epoch: 173, dis loss: 1.385808 , gen loss : 0.737790\n",
      "[Train] epoch: 174, dis loss: 1.385632 , gen loss : 0.737611\n",
      "[Train] epoch: 175, dis loss: 1.385712 , gen loss : 0.737081\n",
      "[Train] epoch: 176, dis loss: 1.382995 , gen loss : 0.738783\n",
      "[Train] epoch: 177, dis loss: 1.383064 , gen loss : 0.738536\n",
      "[Train] epoch: 178, dis loss: 1.381875 , gen loss : 0.739080\n",
      "[Train] epoch: 179, dis loss: 1.380656 , gen loss : 0.739881\n",
      "[Train] epoch: 180, dis loss: 1.379543 , gen loss : 0.741929\n",
      "[Train] epoch: 181, dis loss: 1.379574 , gen loss : 0.741397\n",
      "[Train] epoch: 182, dis loss: 1.375912 , gen loss : 0.743181\n",
      "[Train] epoch: 183, dis loss: 1.373851 , gen loss : 0.744926\n",
      "[Train] epoch: 184, dis loss: 1.369359 , gen loss : 0.751144\n",
      "[Train] epoch: 185, dis loss: 1.364543 , gen loss : 0.752103\n",
      "[Train] epoch: 186, dis loss: 1.359320 , gen loss : 0.752422\n",
      "[Train] epoch: 187, dis loss: 1.379483 , gen loss : 0.744337\n",
      "[Train] epoch: 188, dis loss: 1.376191 , gen loss : 0.745875\n",
      "[Train] epoch: 189, dis loss: 1.375905 , gen loss : 0.746879\n",
      "[Train] epoch: 190, dis loss: 1.368430 , gen loss : 0.750337\n",
      "[Train] epoch: 191, dis loss: 1.362164 , gen loss : 0.753752\n",
      "[Train] epoch: 192, dis loss: 1.349117 , gen loss : 0.763117\n",
      "[Train] epoch: 193, dis loss: 1.380133 , gen loss : 0.747118\n",
      "[Train] epoch: 194, dis loss: 1.379002 , gen loss : 0.746201\n",
      "[Train] epoch: 195, dis loss: 1.376211 , gen loss : 0.747476\n",
      "[Train] epoch: 196, dis loss: 1.371647 , gen loss : 0.749765\n",
      "[Train] epoch: 197, dis loss: 1.367569 , gen loss : 0.751537\n",
      "[Train] epoch: 198, dis loss: 1.389201 , gen loss : 0.736075\n",
      "[Train] epoch: 199, dis loss: 1.388986 , gen loss : 0.737482\n",
      "[Train] epoch: 200, dis loss: 1.388411 , gen loss : 0.736878\n",
      "[Train] epoch: 201, dis loss: 1.387653 , gen loss : 0.736373\n",
      "[Train] epoch: 202, dis loss: 1.387584 , gen loss : 0.735801\n",
      "[Train] epoch: 203, dis loss: 1.387682 , gen loss : 0.735280\n",
      "[Train] epoch: 204, dis loss: 1.387056 , gen loss : 0.734850\n",
      "[Train] epoch: 205, dis loss: 1.387092 , gen loss : 0.734463\n",
      "[Train] epoch: 206, dis loss: 1.386592 , gen loss : 0.734105\n",
      "[Train] epoch: 207, dis loss: 1.386159 , gen loss : 0.733839\n",
      "[Train] epoch: 208, dis loss: 1.385586 , gen loss : 0.733622\n",
      "[Train] epoch: 209, dis loss: 1.385304 , gen loss : 0.733331\n",
      "[Train] epoch: 210, dis loss: 1.385036 , gen loss : 0.733234\n",
      "[Train] epoch: 211, dis loss: 1.384480 , gen loss : 0.733131\n",
      "[Train] epoch: 212, dis loss: 1.384601 , gen loss : 0.733016\n",
      "[Train] epoch: 213, dis loss: 1.383856 , gen loss : 0.732902\n",
      "[Train] epoch: 214, dis loss: 1.383201 , gen loss : 0.732955\n",
      "[Train] epoch: 215, dis loss: 1.383114 , gen loss : 0.732863\n",
      "[Train] epoch: 216, dis loss: 1.383058 , gen loss : 0.732984\n",
      "[Train] epoch: 217, dis loss: 1.382705 , gen loss : 0.733007\n",
      "[Train] epoch: 218, dis loss: 1.382571 , gen loss : 0.733148\n",
      "[Train] epoch: 219, dis loss: 1.382682 , gen loss : 0.733132\n",
      "[Train] epoch: 220, dis loss: 1.381689 , gen loss : 0.733303\n",
      "[Train] epoch: 221, dis loss: 1.380742 , gen loss : 0.680394\n",
      "[Train] epoch: 222, dis loss: 1.382972 , gen loss : 0.734987\n",
      "[Train] epoch: 223, dis loss: 0.599985 , gen loss : 3.258776\n",
      "[Train] epoch: 224, dis loss: 0.042171 , gen loss : 7.674332\n",
      "[Train] epoch: 225, dis loss: 0.024313 , gen loss : 7.605795\n",
      "[Train] epoch: 226, dis loss: 0.019665 , gen loss : 6.776175\n",
      "[Train] epoch: 227, dis loss: 0.015617 , gen loss : 8.913932\n",
      "[Train] epoch: 228, dis loss: 0.013262 , gen loss : 9.102032\n",
      "[Train] epoch: 229, dis loss: 0.012706 , gen loss : 7.612937\n",
      "[Train] epoch: 230, dis loss: 0.012000 , gen loss : 8.132946\n",
      "[Train] epoch: 231, dis loss: 0.011341 , gen loss : 9.095851\n",
      "[Train] epoch: 232, dis loss: 0.010936 , gen loss : 9.219385\n",
      "[Train] epoch: 233, dis loss: 0.010778 , gen loss : 8.808218\n",
      "[Train] epoch: 234, dis loss: 0.010666 , gen loss : 8.357386\n",
      "[Train] epoch: 235, dis loss: 0.032156 , gen loss : 7.893562\n",
      "[Train] epoch: 236, dis loss: 0.015697 , gen loss : 7.207204\n",
      "[Train] epoch: 237, dis loss: 3.628582 , gen loss : 0.151192\n",
      "[Train] epoch: 238, dis loss: 1.375391 , gen loss : 0.823585\n",
      "[Train] epoch: 239, dis loss: 1.348545 , gen loss : 0.836352\n",
      "[Train] epoch: 240, dis loss: 1.357242 , gen loss : 0.830286\n",
      "[Train] epoch: 241, dis loss: 1.334889 , gen loss : 0.905810\n",
      "[Train] epoch: 242, dis loss: 1.389683 , gen loss : 0.808387\n",
      "[Train] epoch: 243, dis loss: 1.387228 , gen loss : 0.807462\n",
      "[Train] epoch: 244, dis loss: 1.388758 , gen loss : 0.803571\n",
      "[Train] epoch: 245, dis loss: 1.388145 , gen loss : 0.801482\n",
      "[Train] epoch: 246, dis loss: 1.388605 , gen loss : 0.798900\n",
      "[Train] epoch: 247, dis loss: 1.388975 , gen loss : 0.796200\n",
      "[Train] epoch: 248, dis loss: 1.387142 , gen loss : 0.794984\n",
      "[Train] epoch: 249, dis loss: 1.388946 , gen loss : 0.791264\n",
      "[Train] epoch: 250, dis loss: 1.385394 , gen loss : 0.792348\n",
      "[Train] epoch: 251, dis loss: 1.386554 , gen loss : 0.789156\n",
      "[Train] epoch: 252, dis loss: 1.385100 , gen loss : 0.788232\n",
      "[Train] epoch: 253, dis loss: 1.383170 , gen loss : 0.787551\n",
      "[Train] epoch: 254, dis loss: 1.382856 , gen loss : 0.786151\n",
      "[Train] epoch: 255, dis loss: 1.382029 , gen loss : 0.784347\n",
      "[Train] epoch: 256, dis loss: 1.382849 , gen loss : 0.783281\n",
      "[Train] epoch: 257, dis loss: 1.382630 , gen loss : 0.781579\n",
      "[Train] epoch: 258, dis loss: 1.378061 , gen loss : 0.782510\n",
      "[Train] epoch: 259, dis loss: 1.382588 , gen loss : 0.779757\n",
      "[Train] epoch: 260, dis loss: 1.374750 , gen loss : 0.781953\n",
      "[Train] epoch: 261, dis loss: 1.424255 , gen loss : 0.792844\n",
      "[Train] epoch: 262, dis loss: 1.380808 , gen loss : 0.823279\n",
      "[Train] epoch: 263, dis loss: 1.385984 , gen loss : 0.773277\n",
      "[Train] epoch: 264, dis loss: 1.386539 , gen loss : 0.772346\n",
      "[Train] epoch: 265, dis loss: 1.376677 , gen loss : 0.775724\n",
      "[Train] epoch: 266, dis loss: 1.369252 , gen loss : 0.779222\n",
      "[Train] epoch: 267, dis loss: 1.384887 , gen loss : 0.786097\n",
      "[Train] epoch: 268, dis loss: 1.380807 , gen loss : 0.772207\n",
      "[Train] epoch: 269, dis loss: 1.377104 , gen loss : 0.773623\n",
      "[Train] epoch: 270, dis loss: 1.374685 , gen loss : 0.774086\n",
      "[Train] epoch: 271, dis loss: 1.366143 , gen loss : 0.778178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 272, dis loss: 1.438726 , gen loss : 0.669432\n",
      "[Train] epoch: 273, dis loss: 1.390349 , gen loss : 0.764823\n",
      "[Train] epoch: 274, dis loss: 1.377946 , gen loss : 0.770078\n",
      "[Train] epoch: 275, dis loss: 1.387002 , gen loss : 0.763811\n",
      "[Train] epoch: 276, dis loss: 1.386519 , gen loss : 0.763224\n",
      "[Train] epoch: 277, dis loss: 1.383255 , gen loss : 0.763840\n",
      "[Train] epoch: 278, dis loss: 1.383986 , gen loss : 0.761962\n",
      "[Train] epoch: 279, dis loss: 1.419216 , gen loss : 0.741341\n",
      "[Train] epoch: 280, dis loss: 1.391592 , gen loss : 0.756506\n",
      "[Train] epoch: 281, dis loss: 1.389952 , gen loss : 0.756011\n",
      "[Train] epoch: 282, dis loss: 1.388891 , gen loss : 0.755466\n",
      "[Train] epoch: 283, dis loss: 1.387570 , gen loss : 0.755186\n",
      "[Train] epoch: 284, dis loss: 1.386727 , gen loss : 0.754996\n",
      "[Train] epoch: 285, dis loss: 1.384513 , gen loss : 0.755326\n",
      "[Train] epoch: 286, dis loss: 1.383948 , gen loss : 0.755439\n",
      "[Train] epoch: 287, dis loss: 1.383123 , gen loss : 0.755076\n",
      "[Train] epoch: 288, dis loss: 1.385405 , gen loss : 0.742346\n",
      "[Train] epoch: 289, dis loss: 1.380459 , gen loss : 0.756751\n",
      "[Train] epoch: 290, dis loss: 1.381611 , gen loss : 0.749539\n",
      "[Train] epoch: 291, dis loss: 1.371810 , gen loss : 0.757419\n",
      "[Train] epoch: 292, dis loss: 1.367323 , gen loss : 0.760396\n",
      "[Train] epoch: 293, dis loss: 1.367301 , gen loss : 0.760426\n",
      "[Train] epoch: 294, dis loss: 1.357691 , gen loss : 0.759126\n",
      "[Train] epoch: 295, dis loss: 1.362609 , gen loss : 0.763461\n",
      "[Train] epoch: 296, dis loss: 1.354007 , gen loss : 0.775815\n",
      "[Train] epoch: 297, dis loss: 1.389643 , gen loss : 0.750854\n",
      "[Train] epoch: 298, dis loss: 1.388536 , gen loss : 0.750743\n",
      "[Train] epoch: 299, dis loss: 1.387828 , gen loss : 0.749511\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train = X_train\n",
    "                    , batch_size=100\n",
    "                    , epoch_num=300\n",
    "                    , savepath = './mnist_5_generate_imgs/'\n",
    "                    , init=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
